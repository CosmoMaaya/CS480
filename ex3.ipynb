{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KandaMaya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG11(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG11, self).__init__()\n",
    "        # convolutional layers\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(512, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.5),\n",
    "            nn.Linear(4096, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        # flatten for FC\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, optimizer, loss_func):\n",
    "    model.train()\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    # For every batch\n",
    "    for i, data in tqdm(enumerate(trainloader), total=len(trainloader)):\n",
    "        image, labels = data\n",
    "        image = image.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        train_running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        train_running_correct += (preds == labels).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    epoch_loss = train_running_loss / len(trainloader)\n",
    "    epoch_acc = (train_running_correct / len(trainloader.dataset))\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, testloader, loss_func):\n",
    "    model.eval()\n",
    "\n",
    "    class_correct = [0.] * 10\n",
    "    class_total = [0.] * 10\n",
    "\n",
    "    valid_running_loss = 0.0\n",
    "    valid_running_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(testloader), total=len(testloader)):\n",
    "            image, labels = data\n",
    "            image = image.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(image)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            valid_running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            valid_running_correct += (preds == labels).sum().item()\n",
    "\n",
    "    epoch_loss = valid_running_loss / len(testloader)\n",
    "    epoch_acc = (valid_running_correct / len(testloader.dataset))\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.style.use('ggplot')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "download_data = True\n",
    "# device = torch.device('cpu')\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((32, 32)),\n",
    "     transforms.ToTensor()])\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "model = VGG11().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# training dataset and data loader\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                             download=download_data, \n",
    "                                             transform=transform)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=True)\n",
    "# validation dataset and dataloader\n",
    "valid_dataset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                           download=download_data, \n",
    "                                           transform=transform)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, \n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=False)\n",
    "                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Epoch 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1875 [00:00<?, ?it/s]c:\\Users\\KandaMaya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:1320: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n",
      "100%|██████████| 1875/1875 [00:18<00:00, 103.71it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 212.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training loss: 1.773, training acc: 0.320\n",
      "Validation loss: 0.592, validation acc: 0.959\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 2 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 263/1875 [00:02<00:13, 120.08it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\KandaMaya\\Documents\\University\\4A\\CS480\\ex3.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/KandaMaya/Documents/University/4A/CS480/ex3.ipynb#ch0000005?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/KandaMaya/Documents/University/4A/CS480/ex3.ipynb#ch0000005?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[INFO]: Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of \u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/KandaMaya/Documents/University/4A/CS480/ex3.ipynb#ch0000005?line=4'>5</a>\u001b[0m     train_epoch_loss, train_epoch_acc \u001b[39m=\u001b[39m train(model, train_dataloader, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/KandaMaya/Documents/University/4A/CS480/ex3.ipynb#ch0000005?line=5'>6</a>\u001b[0m                                               optimizer, loss_func)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/KandaMaya/Documents/University/4A/CS480/ex3.ipynb#ch0000005?line=6'>7</a>\u001b[0m     valid_epoch_loss, valid_epoch_acc \u001b[39m=\u001b[39m validate(model, valid_dataloader,  \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/KandaMaya/Documents/University/4A/CS480/ex3.ipynb#ch0000005?line=7'>8</a>\u001b[0m                                                  loss_func)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/KandaMaya/Documents/University/4A/CS480/ex3.ipynb#ch0000005?line=8'>9</a>\u001b[0m     train_loss\u001b[39m.\u001b[39mappend(train_epoch_loss)\n",
      "\u001b[1;32mc:\\Users\\KandaMaya\\Documents\\University\\4A\\CS480\\ex3.ipynb Cell 3'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, trainloader, optimizer, loss_func)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/KandaMaya/Documents/University/4A/CS480/ex3.ipynb#ch0000002?line=3'>4</a>\u001b[0m train_running_correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/KandaMaya/Documents/University/4A/CS480/ex3.ipynb#ch0000002?line=4'>5</a>\u001b[0m \u001b[39m# For every batch\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/KandaMaya/Documents/University/4A/CS480/ex3.ipynb#ch0000002?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(trainloader), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(trainloader)):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/KandaMaya/Documents/University/4A/CS480/ex3.ipynb#ch0000002?line=6'>7</a>\u001b[0m     image, labels \u001b[39m=\u001b[39m data\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/KandaMaya/Documents/University/4A/CS480/ex3.ipynb#ch0000002?line=7'>8</a>\u001b[0m     image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\KandaMaya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KandaMaya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\KandaMaya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mc:\\Users\\KandaMaya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\KandaMaya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\KandaMaya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\KandaMaya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\KandaMaya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    128\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32mc:\\Users\\KandaMaya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:151\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    150\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n\u001b[1;32m--> 151\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mview(pic\u001b[39m.\u001b[39;49msize[\u001b[39m1\u001b[39;49m], pic\u001b[39m.\u001b[39;49msize[\u001b[39m0\u001b[39;49m], \u001b[39mlen\u001b[39;49m(pic\u001b[39m.\u001b[39;49mgetbands()))\n\u001b[0;32m    152\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    153\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss, valid_loss = [], []\n",
    "train_acc, valid_acc = [], []\n",
    "for epoch in range(epochs):\n",
    "    print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss, train_epoch_acc = train(model, train_dataloader, \n",
    "                                              optimizer, loss_func)\n",
    "    valid_epoch_loss, valid_epoch_acc = validate(model, valid_dataloader,  \n",
    "                                                 loss_func)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    valid_loss.append(valid_epoch_loss)\n",
    "    train_acc.append(train_epoch_acc)\n",
    "    valid_acc.append(valid_epoch_acc)\n",
    "    print('\\n')\n",
    "    print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}\")\n",
    "    print(f\"Validation loss: {valid_epoch_loss:.3f}, validation acc: {valid_epoch_acc:.3f}\")\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_flip(_model):\n",
    "    lr_flip_transform = transforms.Compose(\n",
    "        [transforms.Resize((32, 32)),\n",
    "        transforms.RandomHorizontalFlip(p=1),\n",
    "        transforms.ToTensor()])\n",
    "\n",
    "    td_flip_transform = transforms.Compose(\n",
    "        [transforms.Resize((32, 32)),\n",
    "        transforms.RandomVerticalFlip(p=1),\n",
    "        transforms.ToTensor()])\n",
    "        \n",
    "    # validation dataset and dataloader\n",
    "    lr_flip_dataset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                            download=download_data, \n",
    "                                            transform=lr_flip_transform)\n",
    "    lr_flip_dataloader = torch.utils.data.DataLoader(lr_flip_dataset, \n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=False)\n",
    "\n",
    "    # validation dataset and dataloader\n",
    "    td_flip_dataset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                            download=download_data, \n",
    "                                            transform=td_flip_transform)\n",
    "    td_flip_dataloader = torch.utils.data.DataLoader(td_flip_dataset, \n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=False)\n",
    "\n",
    "    lr_flip_loss, lr_flip_acc = validate(_model, lr_flip_dataloader, loss_func)\n",
    "    td_flip_loss, td_flip_acc = validate(_model, td_flip_dataloader, loss_func)\n",
    "    print(f\"Horizontal flip loss: {lr_flip_loss}, Accuracy: {100. * lr_flip_acc}%\")\n",
    "    print(f\"Vertical flip loss: {td_flip_loss}, Accuracy: {100. * td_flip_acc}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_noise(_model):\n",
    "    for variance in [0.01, 0.1, 1]:\n",
    "        stddev = math.sqrt(variance)\n",
    "        gaussian_noise_transform = transforms.Compose(\n",
    "            [transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x : x + stddev*torch.randn_like(x))\n",
    "            ])\n",
    "            \n",
    "        # validation dataset and dataloader\n",
    "        gaussian_noise_dataset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                                download=download_data, \n",
    "                                                transform=gaussian_noise_transform)\n",
    "        gaussian_noise_dataloader = torch.utils.data.DataLoader(gaussian_noise_dataset, \n",
    "                                                    batch_size=batch_size,\n",
    "                                                    shuffle=False)\n",
    "        gaussian_noise_loss, gaussian_noise_acc = validate(_model, gaussian_noise_dataloader, loss_func)\n",
    "        print(f\"Gaussian noise with variance {variance} loss: {gaussian_noise_loss}, Accuracy: {100. * gaussian_noise_acc}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:02<00:00, 153.55it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 157.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizontal flip loss: 1.928394480635183, Accuracy: 38.43%\n",
      "Vertical flip loss: 1.9220517032062665, Accuracy: 43.309999999999995%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:01<00:00, 171.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian noise with variance 0.01 loss: 0.5477761311081651, Accuracy: 97.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:01<00:00, 177.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian noise with variance 0.1 loss: 2.3260483970276464, Accuracy: 10.780000000000001%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:01<00:00, 163.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian noise with variance 1 loss: 5.4684271934314275, Accuracy: 9.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_flip(model)\n",
    "test_noise(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21bfe1a33d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "gaussian_noise_transform = transforms.Compose(\n",
    "        [transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x : x + 1*torch.randn_like(x))\n",
    "        ])\n",
    "        \n",
    "# validation dataset and dataloader\n",
    "gaussian_noise_dataset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                        download=download_data, \n",
    "                                        transform=gaussian_noise_transform)\n",
    "gaussian_noise_dataloader = torch.utils.data.DataLoader(gaussian_noise_dataset, \n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False)\n",
    "images, labels = next(iter(gaussian_noise_dataloader))\n",
    "plt.imshow(images[4].reshape(32,32), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21b87139600>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gaussian_noise_transform = transforms.Compose(\n",
    "        [transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x : x + math.sqrt(0.1)*torch.randn_like(x))\n",
    "        ])\n",
    "        \n",
    "# validation dataset and dataloader\n",
    "gaussian_noise_dataset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                        download=download_data, \n",
    "                                        transform=gaussian_noise_transform)\n",
    "gaussian_noise_dataloader = torch.utils.data.DataLoader(gaussian_noise_dataset, \n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False)\n",
    "images, labels = next(iter(gaussian_noise_dataloader))\n",
    "plt.imshow(images[4].reshape(32,32), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1995818979.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [12]\u001b[1;36m\u001b[0m\n\u001b[1;33m    plt.imshow(images[4].reshape(32,32), cmap=\"gray\")\\\u001b[0m\n\u001b[1;37m                                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "gaussian_min_noise_transform = transforms.Compose(\n",
    "        [transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x : x + math.sqrt(0.01)*torch.randn_like(x))\n",
    "        ])\n",
    "        \n",
    "# validation dataset and dataloader\n",
    "gaussian_min_noise_dataset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                        download=download_data, \n",
    "                                        transform=gaussian_min_noise_transform)\n",
    "gaussian_min_noise_dataloader = torch.utils.data.DataLoader(gaussian_min_noise_dataset, \n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False)\n",
    "images, labels = next(iter(gaussian_min_noise_dataloader))\n",
    "plt.imshow(images[4].reshape(32,32), cmap=\"gray\")\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_lr_transform = transforms.Compose(\n",
    "    [transforms.Resize((32, 32)),\n",
    "     transforms.RandomHorizontalFlip(p=1),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Lambda(lambda x : x + math.sqrt(0.01)*torch.randn_like(x)),\n",
    "     ])\n",
    "\n",
    "aug_td_transform = transforms.Compose(\n",
    "    [transforms.Resize((32, 32)),\n",
    "     transforms.RandomVerticalFlip(p=1),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Lambda(lambda x : x + math.sqrt(0.01)*torch.randn_like(x)),\n",
    "     ])\n",
    "\n",
    "# training dataset and data loader\n",
    "aug_lr_dataset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                             download=download_data, \n",
    "                                             transform=aug_lr_transform)\n",
    "\n",
    "# training dataset and data loader\n",
    "aug_td_dataset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                             download=download_data, \n",
    "                                             transform=aug_td_transform)\n",
    "\n",
    "\n",
    "aug_loader_cat = torch.utils.data.ConcatDataset([aug_lr_dataset, aug_td_dataset])\n",
    "aug_loader = torch.utils.data.DataLoader(aug_loader_cat, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Epoch 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [00:37<00:00, 99.46it/s] \n",
      "100%|██████████| 313/313 [00:01<00:00, 216.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "aug_training loss: 2.026, aug_training acc: 0.313\n",
      "aug_validation loss: 1.758, aug_validation acc: 0.491\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 2 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [00:37<00:00, 100.70it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 214.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "aug_training loss: 1.833, aug_training acc: 0.327\n",
      "aug_validation loss: 1.748, aug_validation acc: 0.512\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 3 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [00:37<00:00, 99.87it/s] \n",
      "100%|██████████| 313/313 [00:01<00:00, 215.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "aug_training loss: 1.793, aug_training acc: 0.331\n",
      "aug_validation loss: 1.743, aug_validation acc: 0.510\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 4 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [00:36<00:00, 102.06it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 223.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "aug_training loss: 1.783, aug_training acc: 0.328\n",
      "aug_validation loss: 1.795, aug_validation acc: 0.460\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 5 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [00:36<00:00, 103.57it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 221.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "aug_training loss: 1.768, aug_training acc: 0.331\n",
      "aug_validation loss: 1.762, aug_validation acc: 0.487\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 6 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [00:36<00:00, 104.12it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 224.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "aug_training loss: 1.759, aug_training acc: 0.331\n",
      "aug_validation loss: 1.749, aug_validation acc: 0.503\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 7 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [00:35<00:00, 105.11it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 222.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "aug_training loss: 1.761, aug_training acc: 0.329\n",
      "aug_validation loss: 1.891, aug_validation acc: 0.457\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 8 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [00:36<00:00, 102.50it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 217.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "aug_training loss: 1.752, aug_training acc: 0.332\n",
      "aug_validation loss: 1.766, aug_validation acc: 0.469\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 9 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [00:37<00:00, 99.94it/s] \n",
      "100%|██████████| 313/313 [00:01<00:00, 214.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "aug_training loss: 1.750, aug_training acc: 0.331\n",
      "aug_validation loss: 1.694, aug_validation acc: 0.483\n",
      "--------------------------------------------------\n",
      "[INFO]: Epoch 10 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [00:37<00:00, 100.66it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 213.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "aug_training loss: 1.745, aug_training acc: 0.333\n",
      "aug_validation loss: 1.692, aug_validation acc: 0.485\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aug_loss_func = nn.CrossEntropyLoss()\n",
    "aug_model = VGG11().to(device)\n",
    "aug_optimizer = optim.SGD(aug_model.parameters(), lr=0.01)\n",
    "\n",
    "aug_train_loss, aug_valid_loss = [], []\n",
    "aug_train_acc, aug_valid_acc = [], []\n",
    "for epoch in range(epochs):\n",
    "    print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n",
    "    aug_train_epoch_loss, aug_train_epoch_acc = train(aug_model, aug_loader, \n",
    "                                              aug_optimizer, aug_loss_func)\n",
    "    aug_valid_epoch_loss, aug_valid_epoch_acc = validate(aug_model, valid_dataloader,  \n",
    "                                                 aug_loss_func)\n",
    "    aug_train_loss.append(aug_train_epoch_loss)\n",
    "    aug_valid_loss.append(aug_valid_epoch_loss)\n",
    "    aug_train_acc.append(aug_train_epoch_acc)\n",
    "    aug_valid_acc.append(aug_valid_epoch_acc)\n",
    "    print('\\n')\n",
    "    print(f\"aug_training loss: {aug_train_epoch_loss:.3f}, aug_training acc: {aug_train_epoch_acc:.3f}\")\n",
    "    print(f\"aug_validation loss: {aug_valid_epoch_loss:.3f}, aug_validation acc: {aug_valid_epoch_acc:.3f}\")\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_lr_transform = transforms.Compose(\n",
    "    [transforms.Resize((32, 32)),\n",
    "     transforms.RandomHorizontalFlip(p=1),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Lambda(lambda x : x + math.sqrt(0.01)*torch.randn_like(x)),\n",
    "     ])\n",
    "\n",
    "aug_td_transform = transforms.Compose(\n",
    "    [transforms.Resize((32, 32)),\n",
    "     transforms.RandomVerticalFlip(p=1),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Lambda(lambda x : x + math.sqrt(0.01)*torch.randn_like(x)),\n",
    "     ])\n",
    "\n",
    "# training dataset and data loader\n",
    "aug_lr_dataset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                             download=download_data, \n",
    "                                             transform=aug_lr_transform)\n",
    "\n",
    "# training dataset and data loader\n",
    "aug_td_dataset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                             download=download_data, \n",
    "                                             transform=aug_td_transform)\n",
    "\n",
    "\n",
    "aug_loader_cat = torch.utils.data.ConcatDataset([aug_lr_dataset, aug_td_dataset])\n",
    "aug_loader = torch.utils.data.DataLoader(aug_loader_cat, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:02<00:00, 143.94it/s]\n",
      "100%|██████████| 313/313 [00:02<00:00, 144.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizontal flip loss: 0.3185465452008354, Accuracy: 98.75%\n",
      "Vertical flip loss: 0.33421257652413733, Accuracy: 98.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:01<00:00, 159.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian noise with variance 0.01 loss: 1.7181770721563516, Accuracy: 48.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:01<00:00, 183.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian noise with variance 0.1 loss: 1.89471795726508, Accuracy: 44.269999999999996%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:01<00:00, 168.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian noise with variance 1 loss: 2.490477479684848, Accuracy: 14.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_flip(aug_model)\n",
    "test_noise(aug_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f8e04c1004ae0e48c4c128aee02bb34a41f967cffe60616a1dbeccdbed98bf0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
